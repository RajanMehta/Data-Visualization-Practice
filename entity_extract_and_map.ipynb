{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "entity-extract -and-map.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPGCD2HgoOrCcS34Zkr5wRK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajanMehta/nlp-pocs/blob/master/entity_extract_and_map.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOAoywrD_0oO",
        "colab_type": "text"
      },
      "source": [
        "**Problem:**\n",
        "\n",
        "For different types of named-entities that share a semantic position, how can we identify their type?\n",
        "\n",
        "Example:\n",
        "- how much did i spend in `walmart`?\n",
        "- how much did i spend in `new york`?\n",
        "- how much did i spend in `food and drink`?\n",
        "\n",
        "As shown in the example above, in the same sentence structure we have equal chances of having a `merchant`(walmart), a `location`(new york) or a `transaction_category` (food and drink). Our goal is to differentiate these types and map the extracted named-entity correctly.\n",
        "\n",
        "\n",
        "**Why it is difficult to use pre-trained NER models:**\n",
        "\n",
        "- They are case sensitive. Usually they don't work if the query is not properly cased. \n",
        "    - It's highly likely that `how much did i spend in new york?` will not extract `new york` as a `location` entity.\n",
        "    - But if we case it correctly: `How much did i spend in New York?`, it might work just fine. Sadly, this isn't always the case with real-world language data.\n",
        "\n",
        "- They are especially not reliable for the problem described above where you have similar sentence structures but different possible entity types.\n",
        "\n",
        "- There's no way the model would have seen all the possible vocabulary for a particular entity type. Meaning, it might miss some person-entities or some organization-entities. \n",
        "    - This is why we should have a generic model that extracts the entities first followed by having a separate logic that assigns a type to that entity (by validating it with real-world data (wikidata) or by using contextual embeddings (transformers))..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AkoH3K3ICzG",
        "colab_type": "text"
      },
      "source": [
        "#### **Step 1**: Train a completely new entity type.\n",
        "\n",
        "There are many ways to create a custom named-entity recognition model. As google colab comes with spacy, it was super quick to train a very basic model as a proof-of-concept. [reference](https://spacy.io/usage/training#ner)\n",
        "\n",
        "If you run the cell below, spacy will train a new entity called `ENTITY`. \n",
        "Note that the training data has similar sentences with different types of entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyATYzMH3FM1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "43efea63-85bd-4f2a-ef71-a3cb366734c8"
      },
      "source": [
        "import spacy\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import random \n",
        "import datetime as dt\n",
        "\n",
        "model = spacy.load('en_core_web_sm')\n",
        "\n",
        "model.remove_pipe('ner')\n",
        "\n",
        "TRAIN_DATA = [\n",
        "   (\"show new york transactions\", {\"entities\": [(5, 13, \"ENTITY\")]}),\n",
        "   (\"show new jersey transactions\", {\"entities\": [(5, 15, \"ENTITY\")]}),\n",
        "   (\"show walmart transactions\", {\"entities\": [(5, 12, \"ENTITY\")]}),\n",
        "   (\"show grocery transactions\", {\"entities\": [(5, 12, \"ENTITY\")]}),\n",
        "   (\"show entertainment transactions\", {\"entities\": [(5, 18, \"ENTITY\")]}),\n",
        "   (\"show dallas transactions\",{'entities': [(5, 11, 'ENTITY')]}),\n",
        "   (\"show costco transactions\",{'entities': [(5, 11, 'ENTITY')]}),\n",
        "   (\"show pizza hut transactions\",{'entities': [(5, 14, 'ENTITY')]}),\n",
        "   (\"show ann arbor transactions\",{'entities': [(5, 14, 'ENTITY')]}) \n",
        "]\n",
        "\n",
        "def create_blank_nlp(train_data):\n",
        "    nlp = spacy.blank(\"en\")\n",
        "    ner = nlp.create_pipe(\"ner\")\n",
        "    nlp.add_pipe(ner, last=True)\n",
        "    ner = nlp.get_pipe(\"ner\")\n",
        "    for _, annotations in train_data:\n",
        "        for ent in annotations.get(\"entities\"):\n",
        "            ner.add_label(ent[2])\n",
        "    return nlp  \n",
        "\n",
        "nlp = create_blank_nlp(TRAIN_DATA)\n",
        "optimizer = nlp.begin_training()  \n",
        "for i in range(10):\n",
        "    random.shuffle(TRAIN_DATA)\n",
        "    losses = {}\n",
        "    for text, annotations in TRAIN_DATA:\n",
        "        nlp.update([text], [annotations], sgd=optimizer, losses=losses)\n",
        "    print(f\"Losses at iteration {i} - {dt.datetime.now()}\", losses)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Losses at iteration 0 - 2020-08-29 03:11:37.468793 {'ner': 17.837710716063157}\n",
            "Losses at iteration 1 - 2020-08-29 03:11:37.683789 {'ner': 7.221678547572083}\n",
            "Losses at iteration 2 - 2020-08-29 03:11:37.900806 {'ner': 3.752802208555257}\n",
            "Losses at iteration 3 - 2020-08-29 03:11:38.113894 {'ner': 10.005212334148023}\n",
            "Losses at iteration 4 - 2020-08-29 03:11:38.331543 {'ner': 6.78672894530958}\n",
            "Losses at iteration 5 - 2020-08-29 03:11:38.545253 {'ner': 0.3249405558149986}\n",
            "Losses at iteration 6 - 2020-08-29 03:11:38.762234 {'ner': 0.0006758154624136789}\n",
            "Losses at iteration 7 - 2020-08-29 03:11:38.977697 {'ner': 1.2032174909438047e-05}\n",
            "Losses at iteration 8 - 2020-08-29 03:11:39.192019 {'ner': 9.265434348959295e-07}\n",
            "Losses at iteration 9 - 2020-08-29 03:11:39.408050 {'ner': 1.3939076297464356e-07}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3g_GfTm8FFt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3a4a5983-c24b-4167-a09f-0a5282aa40e2"
      },
      "source": [
        "doc = nlp(\"show walmart transactions\")\n",
        "for ent in doc.ents:\n",
        "  print(ent, ent.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "walmart ENTITY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFABiKe5Ou6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNs5ZMhNJZiL",
        "colab_type": "text"
      },
      "source": [
        "#### **Step 2**: Install [txtai](https://github.com/neuml/txtai), an AI-powered search engine built on Transformers. \n",
        "\n",
        "- This will help us map the extracted entity to it's type.\n",
        "- I strongly recommend to go through this [reference](https://towardsdatascience.com/introducing-txtai-an-ai-powered-search-engine-built-on-transformers-37674be252ec) first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDnnHt9yBkiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/neuml/txtai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBmhuvYrBpSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "\n",
        "from txtai.embeddings import Embeddings\n",
        "\n",
        "# Create embeddings model, backed by sentence-transformers & transformers\n",
        "embeddings = Embeddings({\"method\": \"transformers\", \"path\": \"sentence-transformers/bert-base-nli-mean-tokens\"})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaGnVe_FBqDm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sections = [\n",
        "  \"place region location state city country geography\",\n",
        "  \"business organization company enterprise inc institution bank office corporation service\"\n",
        "]\n",
        "\n",
        "# Create an index for the list of sections\n",
        "embeddings.index([(uid, text, None) for uid, text in enumerate(sections)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSWz7BR5V2bL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "232a839b-05ab-425f-d0c6-0c15ccc40e18"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "embeddings.similarity(\"dallas\", sections)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.4549731, 0.3990316], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LIJnSmzWnhw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHiYJEafNV4h",
        "colab_type": "text"
      },
      "source": [
        "#### **Step 3**: Leveraging Wikidata\n",
        "\n",
        "This is not a mandatory step if the embeddings are working fine as per your expectations. \n",
        "\n",
        "- Although embeddings are super powerful, I noticed that it would sometimes map the entity incorrectly. This also depends on how well you define the `sections` above. For example, `disney` is a `merchant` but the transformer thinks that \"place-ness\" of `disney` is higher than its \"merchant-ness\". So, it will assign it the type `place`.\n",
        "- The way I fix this is, instead of searching for `disney`, I search for `what I mean by disney`. By disney, I mean `['film production company', 'media conglomerate', 'commercial organization']`. So, I search for these key words and BAM! the transformer identifies disney as a merchant now.\n",
        "- The way I get that list is by using wikidata. By passing the extracted entity to wikidata, one can get what wikidata instance those entities belong to. It will become easier to understand if you see it first..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSYjfoveB85I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "def get_item(item):\n",
        "    url = 'https://www.wikidata.org/w/api.php'\n",
        "    r = requests.get(url, params = {'format': 'json', 'search': item, 'action': 'wbsearchentities', 'type': 'item', 'language': 'en'})\n",
        "    data = r.json()\n",
        "    return data['search'][0] if len(data['search']) else None\n",
        "\n",
        "def is_instance_of(item):\n",
        "    url = 'https://query.wikidata.org/sparql'\n",
        "    item_obj = get_item(item)\n",
        "    item_id = item_obj[\"id\"] if item_obj else None\n",
        "\n",
        "    if item_id:\n",
        "      query = \"\"\"\n",
        "          SELECT ?ans ?ansLabel\n",
        "          WHERE \n",
        "          {\n",
        "            wd:%s  wdt:P31 ?ans.\n",
        "            SERVICE wikibase:label { bd:serviceParam wikibase:language\n",
        "            \"[AUTO_LANGUAGE],en\". }\n",
        "          }\n",
        "          \"\"\" % (item_id)\n",
        "    \n",
        "      r = requests.get(url, params = {'format': 'json', 'query': query})\n",
        "      r = r.json()\n",
        "      if len(r['results']['bindings']):\n",
        "        result = [obj['ansLabel']['value'] for obj in r['results']['bindings']]\n",
        "      else:\n",
        "        result = None\n",
        "    else:\n",
        "      result = None\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wcVIsO3PPVVq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1a1050e3-0df2-41a3-ef81-62f51bee6d87"
      },
      "source": [
        "is_instance_of(\"disney\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['film production company', 'media conglomerate', 'commercial organization']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-iDlZgsROyW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3045153-6faa-492c-82c0-b072654d6bbd"
      },
      "source": [
        "is_instance_of(\"bofa\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['credit institution', 'business', 'enterprise']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WTrXmKARinC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "9c1e24ee-bb9a-4fcb-b4c1-98f6c9918f11"
      },
      "source": [
        "is_instance_of(\"new york\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['city',\n",
              " 'global city',\n",
              " 'city of the United States',\n",
              " 'big city',\n",
              " 'city with millions of inhabitants',\n",
              " 'port settlement',\n",
              " 'largest city']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQRUE2siRUzX",
        "colab_type": "text"
      },
      "source": [
        "- [Wikidata Tutorial Reference](https://towardsdatascience.com/a-brief-introduction-to-wikidata-bb4e66395eb1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlxgDvD0Rl_V",
        "colab_type": "text"
      },
      "source": [
        "#### **Step 4**: Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz2j5FNzBxra",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "18479b69-ae54-4fc0-ebee-1960133dafb4"
      },
      "source": [
        "import numpy as np \n",
        "\n",
        "queries = [\n",
        "  \"show las vegas transactions\",\n",
        "  \"show chicago transactions\",\n",
        "  \"show pizza hut transactions\",\n",
        "  \"show bofa transactions\",\n",
        "  \"show grocery transactions\",\n",
        "  \"show cinema transactions\"\n",
        "  \"show disney transactions\"        \n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    doc = nlp(query) #ignore the sentense if it doen't make sense :)\n",
        "    for ent in doc.ents:\n",
        "      ent.label_ == \"ENTITY\"\n",
        "      # wikidata instance\n",
        "      entity_type = is_instance_of(ent.text)\n",
        "      if entity_type:\n",
        "        # Get index of best section that best matches query\n",
        "        scores = embeddings.similarity(\" \".join(entity_type), sections)\n",
        "        if np.any(scores > 0.3):\n",
        "          uid = np.argmax(scores)\n",
        "        else:\n",
        "          #transformer isn't sure\n",
        "          uid = \"something-else\"\n",
        "      else:\n",
        "        #wikidata couldn't identify\n",
        "        uid = \"something-else\"\n",
        "      print(\"%-20s %s\" % (ent.text, \"location\" if uid==0 else (\"merchant\" if uid==1 else \"not sure\")))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "las vegas            location\n",
            "chicago              location\n",
            "pizza hut            merchant\n",
            "bofa                 merchant\n",
            "grocery              not sure\n",
            "cinema               not sure\n",
            "disney               merchant\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTPZHogWSvBg",
        "colab_type": "text"
      },
      "source": [
        "#### **Future Work**:\n",
        "- Use a local wikidata dump rather than having an API call to reduce latency. Interesting read: [Kensho](https://blog.kensho.com/announcing-the-kensho-derived-wikimedia-dataset-5d1197d72bcf)\n",
        "- Prepare a test suite. Try different transformer models / fasttext + BM25 as described in the txtai reference above, to compare and select the best one.\n",
        "- Add more sections for even more granular categorization (groceries, restaurants, sports, hobbies, clothes, bills, etc.)"
      ]
    }
  ]
}