{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup   # For HTML parsing\n",
    "import urllib2  # Website connections\n",
    "import re  # Regular expressions\n",
    "from time import sleep  # To prevent overwhelming the server between connections\n",
    "from collections import Counter  # Keep track of our term counts\n",
    "from nltk.corpus import stopwords  # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "import pandas as pd  # For converting results to a dataframe and bar chart plots\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_cleaner(website):\n",
    "    \n",
    "    try:\n",
    "        site = urllib2.urlopen(website).read() # Connect to the job posting\n",
    "    except: \n",
    "        return\n",
    "    \n",
    "    soup_obj = BeautifulSoup(site, 'lxml')\n",
    "    \n",
    "    if len(soup_obj) == 0:    # In case lxml doesn't work, try another one\n",
    "        soup_obj = BeautifulSoup(site, 'html5lib')\n",
    "        \n",
    "    for script in soup_obj([\"script\", \"style\"]):\n",
    "        script.extract() # Remove these two elements from the BS4 object\n",
    "        \n",
    "    text = soup_obj.get_text()\n",
    "    \n",
    "    lines = (line.strip() for line in text.splitlines()) # break into lines\n",
    "    \n",
    "    # break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \")) \n",
    "    \n",
    "    text = ''.join(chunk for chunk in chunks if chunk).encode('utf-8')\n",
    "    \n",
    "    # Now clean out all of the unicode junk\n",
    "    \n",
    "    try:\n",
    "        text = text.decode('unicode_escape').encode('ascii', 'ignore') # Need this as some websites aren't formatted\n",
    "    except:                                                            # in a way that this works, can occasionally throw\n",
    "        return                                                         # an exception\n",
    "        \n",
    "    text = re.sub(\"[^a-zA-Z+3]\",\" \", text)  # Now get rid of any terms that aren't words (include 3 for d3.js)\n",
    "                                             # Also include + for C++\n",
    "    \n",
    "    text = re.sub(r\"([a-z])([A-Z])\", r\"\\1 \\2\", text) # Fix spacing issue from merged words\n",
    "    \n",
    "    text = text.lower().split()  # Go to lower case and split them apart\n",
    "    \n",
    "    stop_words = set(stopwords.words(\"english\")) # Filter out any stop words\n",
    "    text = [w for w in text if not w in stop_words]\n",
    "    \n",
    "    text = list(set(text))\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chaldea',\n",
       " 'consolidated',\n",
       " 'resistance',\n",
       " 'dynasty',\n",
       " 'four',\n",
       " 'barons',\n",
       " 'battle',\n",
       " 'idolatry',\n",
       " 'affairsa']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_cleaner('https://www.vocabulary.com/dictionary/king')[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting page 1\n",
      "Getting page 2\n",
      "Getting page 3\n",
      "Done with collecting the job postings!\n",
      "There were 25 jobs successfully found.\n"
     ]
    }
   ],
   "source": [
    "job_descriptions = [] # Store all our descriptions in this list\n",
    "\n",
    "def skills_info(city, state):\n",
    "    final_job = 'data+scientist'\n",
    "    \n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                   '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "    \n",
    "    final_site = ''.join(final_site_list)\n",
    "    \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    try:\n",
    "        html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8')\n",
    "    job_numbers = re.findall('\\d+', num_jobs_area)\n",
    "    \n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[1])*1000) + int(job_numbers[2])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[1]) \n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "    \n",
    "    num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                  # search result page\n",
    "    \n",
    "    for i in xrange(1,num_pages+1): # Loop through all of our search result pages\n",
    "        print 'Getting page', i\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        html_page = urllib2.urlopen(current_page).read() # Get the page\n",
    "        \n",
    "        page_obj = BeautifulSoup(html_page, 'lxml') # Locate all of the job links\n",
    "        job_link_area = page_obj.find(id = 'resultsCol') # The center column on the page where the job postings exist\n",
    "        \n",
    "        job_URLS = [base_url + link.get('href') for link in job_link_area.find_all('a') if link.get('href') is not None] # Get the URLS for the jobs\n",
    "        \n",
    "        job_URLS = filter(lambda x:'clk' in x, job_URLS) # Now get just the job related URLS\n",
    "        \n",
    "        for j in xrange(0,len(job_URLS)):\n",
    "            final_description = text_cleaner(job_URLS[j])\n",
    "            if final_description: # So that we only append when the website was accessed correctly\n",
    "                job_descriptions.append(final_description)\n",
    "            sleep(1) # So that we don't be jerks. If you have a very fast internet connection you could hit the server a lot! \n",
    "    \n",
    "    print 'Done with collecting the job postings!' \n",
    "    print 'There were', len(job_descriptions), 'jobs successfully found.'\n",
    "    \n",
    "skills_info(city = 'Hartford', state = 'CT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(job, city=None, state=None):\n",
    "    soup_list = []\n",
    "    final_job = job.split()\n",
    "    final_job = '+'.join(word for word in final_job)\n",
    "    \n",
    "    if city is not None:\n",
    "        final_city = city.split() \n",
    "        final_city = '+'.join(word for word in final_city)\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', final_city,\n",
    "                   '%2C+', state] # Join all of our strings together so that indeed will search correctly\n",
    "    elif state is not None:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=%22', final_job, '%22&l=', state]\n",
    "    else:\n",
    "        final_site_list = ['http://www.indeed.com/jobs?q=\"', final_job, '\"']\n",
    "    \n",
    "    final_site = ''.join(final_site_list)\n",
    "    \n",
    "    base_url = 'http://www.indeed.com'\n",
    "    \n",
    "    try:\n",
    "        html = urllib2.urlopen(final_site).read() # Open up the front page of our search first\n",
    "    except:\n",
    "        'That city/state combination did not have any jobs. Exiting . . .' # In case the city is invalid\n",
    "        return\n",
    "    \n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    soup_list.append(soup)\n",
    "    \n",
    "    num_jobs_area = soup.find(id = 'searchCount').string.encode('utf-8')\n",
    "    job_numbers = re.findall('\\d+', num_jobs_area)\n",
    "    \n",
    "    if len(job_numbers) > 3: # Have a total number of jobs greater than 1000\n",
    "        total_num_jobs = (int(job_numbers[1])*1000) + int(job_numbers[2])\n",
    "    else:\n",
    "        total_num_jobs = int(job_numbers[1]) \n",
    "    \n",
    "    city_title = city\n",
    "    if city is None:\n",
    "        city_title = 'Nationwide'\n",
    "    \n",
    "    num_pages = total_num_jobs/10 # This will be how we know the number of times we need to iterate over each new\n",
    "                                  # search result page\n",
    "    \n",
    "    for i in xrange(1,num_pages): # Loop through all of our search result pages\n",
    "        start_num = str(i*10) # Assign the multiplier of 10 to view the pages we want\n",
    "        current_page = ''.join([final_site, '&start=', start_num])\n",
    "        html_page = urllib2.urlopen(current_page).read() # Get the page\n",
    "        page_obj = BeautifulSoup(html_page, 'lxml') # Locate all of the job links\n",
    "        soup_list.append(page_obj)\n",
    "        sleep(1)\n",
    "        \n",
    "    return soup_list\n",
    "\n",
    "def extract_job_title_from_result(soup_list):\n",
    "    jobs = []\n",
    "    for soup in soup_list:\n",
    "        for div in soup.find_all(name=\"div\", attrs={\"class\":\"row\"}):\n",
    "            for a in div.find_all(name=\"a\", attrs={\"data-tn-element\":\"jobTitle\"}):\n",
    "                jobs.append(a[\"title\"])\n",
    "    return(jobs)\n",
    "\n",
    "def extract_company_from_result(soup_list): \n",
    "    companies = []\n",
    "    for soup in soup_list:\n",
    "        for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "            company = div.find_all(name='span', attrs={'class':'company'})\n",
    "            if len(company) > 0:\n",
    "                for b in company:\n",
    "                    companies.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name='span', attrs={'class':'result-link-source'})\n",
    "                for span in sec_try:\n",
    "                    companies.append(span.text.strip())\n",
    "    return(companies)\n",
    "\n",
    "def extract_location_from_result(soup_list):\n",
    "    locations = []\n",
    "    for soup in soup_list:\n",
    "        spans = soup.findAll('span', attrs={'class': 'location'})\n",
    "        for span in spans:\n",
    "            locations.append(span.text.strip())\n",
    "            \n",
    "    return(locations)\n",
    "\n",
    "def extract_summary_from_result(soup_list): \n",
    "    summaries = []\n",
    "    for soup in soup_list:\n",
    "        spans = soup.findAll('span', attrs={'class': 'summary'})\n",
    "        for span in spans:\n",
    "            summaries.append(span.text.strip())\n",
    "    return(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "soup_list = get_soup(job = 'Data Scientist', city = 'Hartford', state = 'CT')\n",
    "\n",
    "data = {\n",
    "    'job_titles' : extract_job_title_from_result(soup_list),\n",
    "    'companies' : extract_company_from_result(soup_list),\n",
    "    'summary' : extract_summary_from_result(soup_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>companies</th>\n",
       "      <th>job_titles</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hartford</td>\n",
       "      <td>Associate Data Scientist - Incubation Lab</td>\n",
       "      <td>Communicate with Data Scientists, Data Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Travelers</td>\n",
       "      <td>Data Scientist- (Machine Learning &amp; Artificial...</td>\n",
       "      <td>Analyze source data, working with structured a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DISNEY</td>\n",
       "      <td>Data Science Manager (Data Scientist)</td>\n",
       "      <td>You will be charged with delivering actionable...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Hartford</td>\n",
       "      <td>Associate Data Scientist - Incubation Lab</td>\n",
       "      <td>Communicate with Data Scientists, Data Enginee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aetna</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Experience with data extraction and analysis. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      companies                                         job_titles  \\\n",
       "0  The Hartford          Associate Data Scientist - Incubation Lab   \n",
       "1     Travelers  Data Scientist- (Machine Learning & Artificial...   \n",
       "2        DISNEY              Data Science Manager (Data Scientist)   \n",
       "3  The Hartford          Associate Data Scientist - Incubation Lab   \n",
       "4         Aetna                                     Data Scientist   \n",
       "\n",
       "                                             summary  \n",
       "0  Communicate with Data Scientists, Data Enginee...  \n",
       "1  Analyze source data, working with structured a...  \n",
       "2  You will be charged with delivering actionable...  \n",
       "3  Communicate with Data Scientists, Data Enginee...  \n",
       "4  Experience with data extraction and analysis. ...  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame().from_dict(data)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
